{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaawansowane Metody Inteligencji Obliczeniowej\n",
    "# Zadanie domowe 1\n",
    "### Prowadzący: Michał Kempka\n",
    "### Autor: Jakub Frąszczak 136704 - SI1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Całe zadanie jest oparte o różne wersje środowiska `FrozenLake` ze znanej biblioteki OpenAI Gym (https://gym.openai.com), która agreguje różnego rodzaju środowiska pod postacią jednego zunifikowanego API.\n",
    "\n",
    "Zapoznaj się z opisem środowiska (https://gym.openai.com/envs/FrozenLake-v0) następnie zapoznaj się z kodem poniżej. Pokazuje on podstawy użytkowania API biblioteki Gym.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod) o ile nie napisano gdzieś inaczej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\jakub\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\jakub\\anaconda3\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\jakub\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\jakub\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\jakub\\anaconda3\\lib\\site-packages (from gym) (1.19.2)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\jakub\\anaconda3\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\jakub\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "# Zainstaluj bibliotekę OpenAI Gym\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Zaimportuj środowisko FrozenLake z OpenAI Gym\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv \n",
    "\n",
    "# Stwórzmy deterministyczne (`is_slippper=False`) środowisko w oparciu o jedną z zpredefiniowanych map (`map_name=\"4x4\"`)\n",
    "env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=False) \n",
    "\n",
    "# Po stworzeniu środowiska musimy je zresetować \n",
    "env.reset()\n",
    "# W każdym momencie możemy wyświetlić stan naszego środowiska przy użyciu fukcji `render`\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przestrzeń akcji:  Discrete(4)\n",
      "Przestrzeń obserwacji:  Discrete(16)\n",
      "Opis środowiska (mapa):\n",
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n",
      "Model przejść w środowisku:\n",
      "{0: {0: [(1.0, 0, 0.0, False)],\n",
      "     1: [(1.0, 4, 0.0, False)],\n",
      "     2: [(1.0, 1, 0.0, False)],\n",
      "     3: [(1.0, 0, 0.0, False)]},\n",
      " 1: {0: [(1.0, 0, 0.0, False)],\n",
      "     1: [(1.0, 5, 0.0, True)],\n",
      "     2: [(1.0, 2, 0.0, False)],\n",
      "     3: [(1.0, 1, 0.0, False)]},\n",
      " 2: {0: [(1.0, 1, 0.0, False)],\n",
      "     1: [(1.0, 6, 0.0, False)],\n",
      "     2: [(1.0, 3, 0.0, False)],\n",
      "     3: [(1.0, 2, 0.0, False)]},\n",
      " 3: {0: [(1.0, 2, 0.0, False)],\n",
      "     1: [(1.0, 7, 0.0, True)],\n",
      "     2: [(1.0, 3, 0.0, False)],\n",
      "     3: [(1.0, 3, 0.0, False)]},\n",
      " 4: {0: [(1.0, 4, 0.0, False)],\n",
      "     1: [(1.0, 8, 0.0, False)],\n",
      "     2: [(1.0, 5, 0.0, True)],\n",
      "     3: [(1.0, 0, 0.0, False)]},\n",
      " 5: {0: [(1.0, 5, 0, True)],\n",
      "     1: [(1.0, 5, 0, True)],\n",
      "     2: [(1.0, 5, 0, True)],\n",
      "     3: [(1.0, 5, 0, True)]},\n",
      " 6: {0: [(1.0, 5, 0.0, True)],\n",
      "     1: [(1.0, 10, 0.0, False)],\n",
      "     2: [(1.0, 7, 0.0, True)],\n",
      "     3: [(1.0, 2, 0.0, False)]},\n",
      " 7: {0: [(1.0, 7, 0, True)],\n",
      "     1: [(1.0, 7, 0, True)],\n",
      "     2: [(1.0, 7, 0, True)],\n",
      "     3: [(1.0, 7, 0, True)]},\n",
      " 8: {0: [(1.0, 8, 0.0, False)],\n",
      "     1: [(1.0, 12, 0.0, True)],\n",
      "     2: [(1.0, 9, 0.0, False)],\n",
      "     3: [(1.0, 4, 0.0, False)]},\n",
      " 9: {0: [(1.0, 8, 0.0, False)],\n",
      "     1: [(1.0, 13, 0.0, False)],\n",
      "     2: [(1.0, 10, 0.0, False)],\n",
      "     3: [(1.0, 5, 0.0, True)]},\n",
      " 10: {0: [(1.0, 9, 0.0, False)],\n",
      "      1: [(1.0, 14, 0.0, False)],\n",
      "      2: [(1.0, 11, 0.0, True)],\n",
      "      3: [(1.0, 6, 0.0, False)]},\n",
      " 11: {0: [(1.0, 11, 0, True)],\n",
      "      1: [(1.0, 11, 0, True)],\n",
      "      2: [(1.0, 11, 0, True)],\n",
      "      3: [(1.0, 11, 0, True)]},\n",
      " 12: {0: [(1.0, 12, 0, True)],\n",
      "      1: [(1.0, 12, 0, True)],\n",
      "      2: [(1.0, 12, 0, True)],\n",
      "      3: [(1.0, 12, 0, True)]},\n",
      " 13: {0: [(1.0, 12, 0.0, True)],\n",
      "      1: [(1.0, 13, 0.0, False)],\n",
      "      2: [(1.0, 14, 0.0, False)],\n",
      "      3: [(1.0, 9, 0.0, False)]},\n",
      " 14: {0: [(1.0, 13, 0.0, False)],\n",
      "      1: [(1.0, 14, 0.0, False)],\n",
      "      2: [(1.0, 15, 1.0, True)],\n",
      "      3: [(1.0, 10, 0.0, False)]},\n",
      " 15: {0: [(1.0, 15, 0, True)],\n",
      "      1: [(1.0, 15, 0, True)],\n",
      "      2: [(1.0, 15, 0, True)],\n",
      "      3: [(1.0, 15, 0, True)]}}\n",
      "Aktualny stan:  0\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Najważniejsze pola środowiska, na potrzeby tego zadania załóżmy, że mamy dostęp do nich wszystkich \n",
    "# (oczywiście dla niektórych środowisk w OpenAI Gym tak nie jest)\n",
    "print(\"Przestrzeń akcji: \", env.action_space) # Dyskretne akcje od 0 do 3: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n",
    "print(\"Przestrzeń obserwacji: \", env.observation_space) # Dyskretne stany od 0 do 15\n",
    "print(\"Opis środowiska (mapa):\")\n",
    "print(env.desc)\n",
    "print(\"Model przejść w środowisku:\")\n",
    "pprint(env.P) # gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "print(\"Aktualny stan: \", env.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Nasz agent może wejść w interakcje ze środowiskiem  poprzez wywołanie funkcji `step(action)`, \n",
    "# gdzie `action` to jedna z możliwych akcji (int od 0 do env.action_space.n - 1)\n",
    "s = env.reset() # `reset()` zwraca początkowy stan\n",
    "env.render()\n",
    "for i in range(5):\n",
    "    # Wybierzmy losową akcje\n",
    "    random_a = env.action_space.sample() \n",
    "    # `step(action)` zwraca nowy stan (`s`), nagrodę (`r`), informację czy stan jest terminalny (`term`) \n",
    "    # oraz dodatkowe informacje, które pomijamy\n",
    "    # w tym wypadku nowy stan to jedynie id, ale dla innych środowisk może być to inny typ reprezentujący obserwację\n",
    "    s, r, term, _ = env.step(random_a) \n",
    "    env.render()\n",
    "    if term:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 1 - Policy iteration + value iteration (10 pkt)\n",
    "\n",
    "W komórkach poniżej zaimplementuj algorytmy **iteracji polityki** oraz **iteracji wartości**, wyznaczające deterministyczną politykę dla środowiska FrozenLake.\n",
    "\n",
    "Odpowiedź na pytania wykonując odpowiednie eksperymenty (zostaw output odpowiednich komórek na poparcie swoich twierdzeń):\n",
    "- Jak zmiana współczynniku `gamma` wpływa na wynikową politykę?\n",
    "- Jak stochastyczność wpływa na liczbę iteracji potrzebnych do zbiegnięcia obu algorytmów oraz wynikową politykę?\n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy funkcji `policy_iteration` i `value_iteration`, ani ich argumentów. Nie dopisuj do komórek z funkcjami innego kodu. Może zdefiniować funkcje pomocnicze dla danej funkcji w tej samej komórce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedzi: Miejsce na Twoje odpowiedzi\n",
    "\n",
    "1) Im współczynnik gamma jest większy tym większą wagę mają oczekiwane nagrody z przyszłości. Gdyby współczynnik gamma wynosił 0, polityka byłaby bardzo krótkowzroczna ponieważ patrzyłaby jedynie na nagrodę z następnego ruchu.\n",
    "\n",
    "2) Wraz ze wzrostem stochastyczności środowiska algorytm iteracji wartości potrzebuje większej liczby iteracji by się zbiec, natomiast dla algorytmu iteracji polityki stochastyczność nie rzutuje na jego zbieżność. Wzrost stochastyczności uniemożliwia uzyskanie polityki, która zawsze osiąga optymalną nagrodę, możliwe jest jedynie maksymalizowanie wartości oczekiwanej nagrody. W kontekście powyższego środowiska, ustawienie gamma na 0, oczywiście nie miałoby sensu ponieważ finalnym celem jest dotarcie do docelowego miejsca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, gamma, delta=0.001):\n",
    "    \"\"\"\n",
    "    Argumenty:\n",
    "        P - model przejścia, gdzie P[s][a] == [(prawdopodobieństwo, kolejny stan, nagroda, czy stan terminalny), ...]\n",
    "        gamma - współczynnik dyskontujący\n",
    "        delta - tolerancja warunku stopu\n",
    "    Zwracane wartości:\n",
    "        V - lista o długości len(P) zawierający oszacowane wartość stanu s: V[s]\n",
    "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
    "        i - ilość iteracji algorytmu po wszystkich stanach\n",
    "    \"\"\"\n",
    "    \n",
    "    def bellman_equation(P, V, pi, gamma, s, a=None):\n",
    "        if a is None:\n",
    "            a = pi[s]\n",
    "        expected_value = 0\n",
    "        for state in P[s][a]:\n",
    "            probability, next_state, reward, termination = state\n",
    "            expected_value += probability * (reward + gamma * V[next_state])\n",
    "        return expected_value\n",
    "\n",
    "    def policy_evaluation(P, V, pi, gamma, delta_min):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(len(V)):\n",
    "                v = V[s]\n",
    "                V[s] = bellman_equation(P, V, pi, gamma, s)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < delta_min:\n",
    "                return V\n",
    "\n",
    "    def max_action(P, V, pi, gamma, s):\n",
    "        actions = [action for action, _ in P[0].items()]\n",
    "        best_action = actions[0]\n",
    "        max_value = bellman_equation(P, V, pi, gamma, s, a=best_action)\n",
    "\n",
    "        for action in actions[1:]:\n",
    "            value = bellman_equation(P, V, pi, gamma, s, a=action)\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def policy_update(P, V, pi, gamma):\n",
    "        stop = True\n",
    "        for s in range(len(V)):\n",
    "            a = pi[s]\n",
    "            pi[s] = max_action(P, V, pi, gamma, s) \n",
    "            if a != pi[s]:\n",
    "                stop = False\n",
    "        return pi, stop\n",
    "    \n",
    "    \n",
    "    V = [0] * len(P)\n",
    "    pi = [0] * len(P)\n",
    "    i = 0\n",
    "    \n",
    "    stop = False\n",
    "    while not stop:\n",
    "        V = policy_evaluation(P, V, pi, gamma, delta)\n",
    "        pi, stop = policy_update(P, V, pi, gamma)\n",
    "        i += 1\n",
    "               \n",
    "    return V, pi, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma, delta=0.001):\n",
    "    \"\"\"\n",
    "    Argumenty:\n",
    "        P - model przejścia, gdzie P[s][a] == [(prawdopodobieństwo, kolejny stan, nagroda, czy stan terminalny), ...]\n",
    "        gamma - współczynnik dyskontujący\n",
    "        delta - tolerancja warunku stopu\n",
    "    Zwracane wartości:\n",
    "        Q - lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
    "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
    "        i - ilość iteracji algorytmu po wszystkich stanach\n",
    "    \"\"\"\n",
    "    def bellman_equation(P, Q, pi, gamma, s, a=None):\n",
    "        if a is None:\n",
    "            a = pi[s]\n",
    "        expected_value = 0\n",
    "        for state in P[s][a]:\n",
    "            probability, next_state, reward, termination = state\n",
    "            expected_value += probability * (reward + gamma * max(Q[next_state]))\n",
    "        return expected_value\n",
    "\n",
    "    def values_update(P, Q, pi, gamma, delta_min):\n",
    "        i = 0\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(len(Q)):\n",
    "                v = max(Q[s])\n",
    "                for action in range(len(Q[s])):\n",
    "                    Q[s][action] = bellman_equation(P, Q, pi, gamma, s, a=action)\n",
    "                delta = max(delta, abs(v - max(Q[s])))\n",
    "            i += 1\n",
    "            if delta < delta_min:\n",
    "                return Q, i\n",
    "\n",
    "    def max_action(Q, s):\n",
    "        best_action = 0\n",
    "        best_value = Q[s][0]\n",
    "        for action in range(1, len(Q[s])):\n",
    "            if Q[s][action] > best_value:\n",
    "                best_value = Q[s][action]\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def policy_update(P, Q, pi, gamma):\n",
    "        for s in range(len(Q)):\n",
    "            pi[s] = max_action(Q, s)\n",
    "        return pi\n",
    "    \n",
    "    pi = [0] * len(P)\n",
    "    Q = [[0] * len(P[s]) for s in P.keys()]\n",
    "    i = 0\n",
    "    \n",
    "    Q, i = values_update(P, Q, pi, gamma, delta)\n",
    "    pi = policy_update(P, Q, pi, gamma)\n",
    "    \n",
    "    return Q, pi, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFHFFH\n",
      "FFHFFFFF\n",
      "FFFFFHHF\n",
      "HFHFFFFF\n",
      "FFFFFHFH\n",
      "FFFFFFHF\n",
      "FFFFFFFF\n",
      "FFFFFHFG\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "# Przykładowy kod do testowania zaimplementowanych metod\n",
    "\n",
    "# Zaimportuj generator map dla środowiska FrozenLake z OpenAI Gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "# Wygeneruj losową mapę jeziora o zadanym rozmiarze (`size=`)\n",
    "lake_map = generate_random_map(size=8)\n",
    "\n",
    "# Stwórz środowisko w oparciu o wygenerowaną mapę, \n",
    "# sprawdz deterministyczną (`is_slippery=False`) jak i stochastyczną wersję środowiska (`is_slippery=True`)\n",
    "env = FrozenLakeEnv(desc=lake_map, is_slippery=True) \n",
    "env.render()\n",
    "\n",
    "# Ciekawym przykładem jest też jedna z predefiniowanych map \"8x8\"\n",
    "env = FrozenLakeEnv(map_name=\"8x8\", is_slippery=True) \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wprowadzmy teraz funkcję, które empirycznie zewauluje naszą politykę\n",
    "# po prostu rozgrywając odpowiednią liczbę episodów zgodnie z naszą polityką.\n",
    "def evaluate_empirically(env, pi, episodes=1000, max_actions=100):\n",
    "    mean_r = 0\n",
    "    for e in range(episodes):\n",
    "        s = env.reset()\n",
    "        total_r = 0\n",
    "        for _ in range(max_actions): # Na wypadek polityki, która nigdy nie dojdzie od stanu terminalnego\n",
    "            s, r, final, _ = env.step(pi[s])\n",
    "            total_r += r\n",
    "            if final:\n",
    "                break\n",
    "        mean_r += 1/(e + 1) * (total_r - mean_r)\n",
    "    return mean_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "15 15\n",
      "1.0\n",
      "1.0\n",
      "7 7\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeEnv(map_name=\"8x8\", is_slippery=False)\n",
    "V, pi1, i1 = policy_iteration(env.P, 0.9)\n",
    "Q, pi2, i2 = value_iteration(env.P, 0.9)\n",
    "print(evaluate_empirically(env, pi1))\n",
    "print(evaluate_empirically(env, pi2))\n",
    "print(i1, i2)\n",
    "\n",
    "env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=False)\n",
    "V, pi1, i1 = policy_iteration(env.P, 0.9)\n",
    "Q, pi2, i2 = value_iteration(env.P, 0.9)\n",
    "print(evaluate_empirically(env, pi1))\n",
    "print(evaluate_empirically(env, pi2))\n",
    "print(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5750000000000006\n",
      "0.547\n",
      "12 21\n",
      "0.728\n",
      "0.7290000000000002\n",
      "6 21\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeEnv(map_name=\"8x8\", is_slippery=True)\n",
    "V, pi1, i1 = policy_iteration(env.P, 0.9)\n",
    "Q, pi2, i2 = value_iteration(env.P, 0.9)\n",
    "print(evaluate_empirically(env, pi1))\n",
    "print(evaluate_empirically(env, pi2))\n",
    "print(i1, i2)\n",
    "\n",
    "env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=True)\n",
    "V, pi1, i1 = policy_iteration(env.P, 0.9)\n",
    "Q, pi2, i2 = value_iteration(env.P, 0.9)\n",
    "print(evaluate_empirically(env, pi1))\n",
    "print(evaluate_empirically(env, pi2))\n",
    "print(i1, i2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 2 - Monte Carlo (10 pkt)\n",
    "W komórce poniżej zaimplementuj metodę **On-policy Monte Carlo** dla polityki epsilon-greedy.\n",
    "Zakładamy, że model przejść nie jest w tym wypadku dla nas dostępny,\n",
    "dlatego możesz używać wyłącznie metod `env.reset()` i `env.step()`\n",
    "w swojej implementacji, w celu wygenerowania nowego epizodu.\n",
    "\n",
    "- Zaproponuj warunek stopu dla swojej implementacji.\n",
    "- Jaki jest wpływ epsilony na działanie algorytmu?\n",
    "- Jaka prosta modyfikacja nagród środowiska przyśpieszyłaby odkrywanie dobrej polityki? Zmodyfikuj env.P i zademonstruj. \n",
    "\n",
    "Tip: z racji, że env.P jest dostępne, możesz porównać wyniki `on_policy_eps_greedy_monte_carlo` ze wynikami `value_iteration`. \n",
    "\n",
    "#### Uwaga: nie zmieniaj nazwy funkcji `on_policy_eps_greedy_monte_carlo`, ani jej pierwszych argumentów (możesz dodać nowe argumenty z wartościami domyślnymi). Nie dopisuj do komórki z funkcją innego kodu. Może zdefiniować funkcje pomocnicze dla funkcji w tej samej komórce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedź: Miejsce na Twoje odpowiedzi\n",
    "\n",
    "1) W celu sprawdzenia czy zakończyć działanie algorytmu przechowywana jest historia k ostatnich wygenerowanych epizodów. Jeżeli odpowiedni procent, determinowany współczynnikiem epsilon, tych epizodów zakończyła się sukcesem, czyli dotarciem do celu to można założyć, że obliczana polityka jest wystarczająco dobra ponieważ w sposób powtarzalny dociera do celu, pomimo swojej wbudowanej stochastyczności.\n",
    "\n",
    "2) Im epsilon jest większy tym algorytm cechuje się większą zdolnością eksploracyjną a więc zostaną wygenerowane bardziej różnorodne epizody umożliwiające w szerszym stopniu oszacowanie wartości oczekiwanej nagrody. Jest to zatem pożądana własność na początku działania algorytmu, jednakże na późniejszym etapie spowoduje to ignorowanie wiedzy o już oszacowanych nagrodach oczekiwanych. Pod koniec działania epsilon powinien zatem maleć aby algorytm oscylował bardziej wokół sub-optymalnych rozwiązań i był w stanie skuteczniej oczacować wartości par stanów i akcji koniecznych do dotarcia do celu.\n",
    "\n",
    "3) W oryginalnym środowisku nagroda jest przyznawana jedynie za osiągnięcie celu a zatem oszacowanie wartości oczekiwanej dla danego stanu i akcji propaguje się jedynie od tego stanu docelowego. Z tego powodu duża część wygenerowanych rozwiązań jest jedynie ślepym błądzeniem. W celu przyspieszenia odkrywania dobrej polityki mogłyby zostać przydzielane nagrody za zbliżenie się do docelowego miejsca. Dzięki temu algorytm priorytetyzowałby od samego początku konkretne akcje, nawet jeśli nie został wygenerowany jeszcze epizod zawierający stan docelowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Step:\n",
    "    def __init__(self, state, action, reward):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "    \n",
    "    def the_same(self, state, action):\n",
    "        return state == self.state and action == self.action\n",
    "    \n",
    "    def values(self):\n",
    "        return self.state, self.action, self.reward\n",
    "    \n",
    "    def show(self):\n",
    "        return str(self.state) + ' ' + str(self.action) + ' ' + str(self.reward)\n",
    "        \n",
    "def choose_action(probabilities):\n",
    "    p = random.uniform(0, 1)\n",
    "    for action in range(len(probabilities)):\n",
    "        action_probability = sum(probabilities[:action + 1])\n",
    "        if action_probability >= p:\n",
    "            return action\n",
    "\n",
    "def generate_episode(env, policy):\n",
    "    env.reset()\n",
    "    state = 0\n",
    "    episode = []\n",
    "    termination = False\n",
    "    while not termination:\n",
    "        action = choose_action(policy[state])\n",
    "        next_state, reward, termination, _ = env.step(action) \n",
    "        step = Step(state, action, reward)\n",
    "        episode.append(step)\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "def first_visit(episode, t):\n",
    "    state, action, _ = episode[t].values()\n",
    "    for i in range(t):\n",
    "        step = episode[i]\n",
    "        if step.the_same(state, action):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def mean(array):\n",
    "    return sum(array) / len(array)\n",
    "\n",
    "def max_action(Q, state):\n",
    "    best_action = None\n",
    "    best_value = 0\n",
    "    for action, value in enumerate(Q[state]):\n",
    "        if best_action is None or value > best_value:\n",
    "            best_action = action\n",
    "            best_value = value\n",
    "    return best_action\n",
    "\n",
    "def convert_to_deterministic_policy(nd_policy, pi):\n",
    "    for state in range(len(nd_policy)):\n",
    "        action = nd_policy[state].index(max(nd_policy[state]))\n",
    "        pi[state] = action\n",
    "    return pi\n",
    "\n",
    "def show_episode(episode):\n",
    "    line = ''\n",
    "    for step in episode:\n",
    "        line += step.show()\n",
    "        line += '-'\n",
    "    print(line)\n",
    "    \n",
    "def stop_condition(episodes_history, history_length, epsilon):\n",
    "    k = 0\n",
    "    mean_length = 0\n",
    "    if len(episodes_history) == history_length:\n",
    "        for episode in episodes_history:\n",
    "            k += episode[-1].reward\n",
    "            mean_length += len(episode)\n",
    "        mean_length /= history_length\n",
    "        return k >= (1 - epsilon) * history_length\n",
    "    return False\n",
    "        \n",
    "def on_policy_eps_greedy_monte_carlo(env, eps, gamma):\n",
    "    \"\"\"\n",
    "    Argumenty:\n",
    "        env - środowisko implementujące metody `reset()` oraz `step(action)`\n",
    "        eps - współczynnik eksploracji\n",
    "        gamma - współczynnik dyskontujący\n",
    "    Zwracane wartości:\n",
    "        Q - lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
    "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną (zachłanną) politykę - akcję dla stanu s: pi[s]\n",
    "        i - ilość epizodów wygenerowanych przez algorytm\n",
    "    \"\"\"\n",
    "    P = env.P\n",
    "    \n",
    "    pi = [0] * len(P)\n",
    "    Q = [[0] * len(P[s]) for s in P.keys()]\n",
    "    i = 0\n",
    "    \n",
    "    returns = []\n",
    "    for s in P.keys():\n",
    "        tmp = []\n",
    "        for _ in range(len(P[s])):\n",
    "            tmp.append([])\n",
    "        returns.append(tmp)\n",
    "    \n",
    "    nd_policy = [[1 + eps * (1 / len(P[s]) - 1) if i == 0 else eps / len(P[s]) for i in range(len(P[s]))] for s in P.keys()]\n",
    "    \n",
    "    episodes_history = []\n",
    "    history_length = 10\n",
    "    \n",
    "    while not stop_condition(episodes_history, history_length, eps):\n",
    "        episode = generate_episode(env, nd_policy)\n",
    "        \n",
    "        episodes_history.insert(0, episode)\n",
    "        episodes_history = episodes_history[:min(history_length, len(episodes_history))]\n",
    "        \n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t].values()\n",
    "            G = gamma * G + reward\n",
    "            if first_visit(episode, t):\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = mean(returns[state][action])\n",
    "\n",
    "                A_star = max_action(Q, state)\n",
    "\n",
    "                for a in range(len(Q[state])):\n",
    "                    if a == A_star:\n",
    "                        nd_policy[state][a] = 1 + eps * (1 / len(Q[state]) - 1)\n",
    "                    else:\n",
    "                        nd_policy[state][a] = eps / len(Q[state])\n",
    "        i += 1\n",
    "    \n",
    "    pi = convert_to_deterministic_policy(nd_policy, pi)\n",
    "    \n",
    "    return Q, pi, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_environment(env):\n",
    "    for state in range(len(env.P)):\n",
    "        for action in range(len(env.P[state])):\n",
    "            for option in range(len(env.P[state][action])):\n",
    "                if not env.P[state][action][option][-1] and env.P[state][action][option][-2] == 0 and (action == 1 or action == 2):\n",
    "                    if env.P[state][action][option][1] != state:\n",
    "                        tmp = list(env.P[state][action][option][:])\n",
    "                        tmp[2] = 0.1\n",
    "                        tmp = tuple(tmp)\n",
    "                        env.P[state][action][option] = tmp[:]\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3210000000000003"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_slippery = True\n",
    "env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=is_slippery) \n",
    "env.reset()\n",
    "Q, pi, i = on_policy_eps_greedy_monte_carlo(env, 0.7, 0.9)\n",
    "\n",
    "env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=is_slippery) \n",
    "env.reset()\n",
    "evaluate_empirically(env, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia liczba iteracji bez konwersji środowiska: 274.15\n",
      "Średnia liczba iteracji z konwersją środowiska: 120.45\n"
     ]
    }
   ],
   "source": [
    "no_conversion = 0\n",
    "conversion = 0\n",
    "num = 20\n",
    "for _ in range(num):\n",
    "    is_slippery = False\n",
    "    env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=is_slippery) \n",
    "    env.reset()\n",
    "    Q, pi, i = on_policy_eps_greedy_monte_carlo(env, 0.7, 0.9)\n",
    "    no_conversion += i\n",
    "    \n",
    "    env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=is_slippery) \n",
    "    env.reset()\n",
    "    env = convert_environment(env)\n",
    "    Q, pi, i = on_policy_eps_greedy_monte_carlo(env, 0.7, 0.9)\n",
    "    conversion += i\n",
    "\n",
    "print('Średnia liczba iteracji bez konwersji środowiska:', no_conversion / num)\n",
    "print('Średnia liczba iteracji z konwersją środowiska:', conversion / num)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
